{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고: Interview with youngest kaggle grandmaster\n",
    "\n",
    "https://hackernoon.com/interview-with-the-youngest-kaggle-grandmaster-mikel-bober-irizar-anokas-17dfd2461070?fbclid=IwAR1J1Dv3GCLy82klgH4LY_QCC7yjQiIfaD7N9MgqdtXzPw5c87hRyIPc0Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Little Boat - 2nd Place Lightgbm Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/base-workspace/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgbm\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    # sort rows on prediction column\n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_only = True\n",
    "save_cv = True\n",
    "full_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_loc = '/Users/lifesailor/.kaggle/porto-seguro/'\n",
    "\n",
    "train = pd.read_csv(folder_loc + 'train.csv')\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "\n",
    "test = pd.read_csv(folder_loc + 'test.csv')\n",
    "test_id = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "y = train['target'].values\n",
    "drop_features = ['id', 'target']\n",
    "X = train.drop(drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist()\n",
    "\n",
    "cat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\n",
    "num_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature Selection - calc 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_10_cat',\n",
       " 'ps_car_11_cat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_01',\n",
       " 'ps_ind_03',\n",
       " 'ps_ind_06_bin',\n",
       " 'ps_ind_07_bin',\n",
       " 'ps_ind_08_bin',\n",
       " 'ps_ind_09_bin',\n",
       " 'ps_ind_10_bin',\n",
       " 'ps_ind_11_bin',\n",
       " 'ps_ind_12_bin',\n",
       " 'ps_ind_13_bin',\n",
       " 'ps_ind_14',\n",
       " 'ps_ind_15',\n",
       " 'ps_ind_16_bin',\n",
       " 'ps_ind_17_bin',\n",
       " 'ps_ind_18_bin',\n",
       " 'ps_reg_01',\n",
       " 'ps_reg_02',\n",
       " 'ps_reg_03',\n",
       " 'ps_car_11',\n",
       " 'ps_car_12',\n",
       " 'ps_car_13',\n",
       " 'ps_car_14',\n",
       " 'ps_car_15']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 결측치 Feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "num_features.append('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c])\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/base-workspace/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(train[cat_features])\n",
    "X_cat = enc.transform(train[cat_features])\n",
    "X_t_cat = enc.transform(test[cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new_ind feature - ind feature 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_features = [c for c in feature_names if 'ind' in c]\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_01',\n",
       " 'ps_ind_02_cat',\n",
       " 'ps_ind_03',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_ind_06_bin',\n",
       " 'ps_ind_07_bin',\n",
       " 'ps_ind_08_bin',\n",
       " 'ps_ind_09_bin',\n",
       " 'ps_ind_10_bin',\n",
       " 'ps_ind_11_bin',\n",
       " 'ps_ind_12_bin',\n",
       " 'ps_ind_13_bin',\n",
       " 'ps_ind_14',\n",
       " 'ps_ind_15',\n",
       " 'ps_ind_16_bin',\n",
       " 'ps_ind_17_bin',\n",
       " 'ps_ind_18_bin']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ind_features:\n",
    "    if count == 0:\n",
    "        train['new_ind'] = train[c].astype(str) + \"_\"\n",
    "        test['new_ind'] = test[c].astype(str) + \"_\"\n",
    "        count += 1\n",
    "    else:\n",
    "        train['new_ind'] += train[c].astype(str) + \"_\"\n",
    "        test['new_ind'] += test[c].astype(str) + \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_2_5_2_1_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
      "0_1_8_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_\n"
     ]
    }
   ],
   "source": [
    "print(train['new_ind'][0])\n",
    "print(test['new_ind'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. cat_count feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, 0: 523}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = cat_features[0]\n",
    "pd.concat([train[c], test[c]]).value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docstring: D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_2_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 2992,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 2784,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 2568,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 2174,\n",
       " '0_2_0_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 2131,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 2061,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 2030,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1945,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1898,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1878,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1815,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1809,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1750,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1635,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1619,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1611,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1543,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 1527,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 1493,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1492,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1424,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1401,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 1372,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1352,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 1330,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1293,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1292,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 1265,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1254,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1208,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1195,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1180,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1175,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1175,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1174,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1147,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 1141,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1113,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 1107,\n",
       " '0_2_0_1_0_1_0_0_0_0_0_0_0_0_7_1_0_0_': 1085,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1076,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 1069,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 1061,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 1059,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 1058,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1042,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1041,\n",
       " '0_2_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 1018,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 1010,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 1001,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 1001,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 996,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 987,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 983,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 978,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 976,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 973,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 970,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 965,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 961,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 957,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 956,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 953,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 952,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 951,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 938,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 933,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 929,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 925,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 921,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 910,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 910,\n",
       " '1_2_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 898,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 869,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 866,\n",
       " '1_2_0_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 861,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 856,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 851,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 846,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 842,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 840,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 834,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 833,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 833,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 829,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 828,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 828,\n",
       " '0_2_0_1_7_1_0_0_0_0_0_0_0_0_7_1_0_0_': 826,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 823,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 820,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 819,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 818,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 815,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 806,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 802,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 797,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 792,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 792,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 782,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 781,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 780,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 778,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 777,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 776,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 774,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 773,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 758,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 753,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 751,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 748,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 744,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 742,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 734,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 729,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 726,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 725,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 720,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 718,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 717,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 714,\n",
       " '0_2_1_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 712,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 710,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 710,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 706,\n",
       " '1_2_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 704,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 702,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 701,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 699,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 699,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 693,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 690,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 690,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 690,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 686,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 685,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 685,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 684,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 683,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 681,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 676,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 674,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 673,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 672,\n",
       " '1_1_1_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 665,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 660,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 660,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 659,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 653,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 652,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 652,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 650,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 647,\n",
       " '1_2_1_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 641,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 641,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 640,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 638,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 632,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 630,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 629,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 624,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 622,\n",
       " '2_2_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 619,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 615,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 615,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 613,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 613,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 608,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 608,\n",
       " '2_2_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 605,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 599,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 596,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 594,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 593,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 589,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 586,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 585,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 580,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 574,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 571,\n",
       " '0_2_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 570,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 566,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 565,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 564,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 561,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 561,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 561,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 558,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 552,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 551,\n",
       " '1_2_1_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 548,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 548,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 545,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 545,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 544,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 544,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 543,\n",
       " '1_2_1_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 542,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 541,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 539,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 537,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 537,\n",
       " '1_2_1_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 536,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 535,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 533,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 533,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 530,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 529,\n",
       " '0_2_0_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 528,\n",
       " '0_2_2_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 527,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 526,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 525,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 523,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 522,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 518,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 517,\n",
       " '1_1_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 517,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 514,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 513,\n",
       " '1_2_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 512,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 510,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 510,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 506,\n",
       " '0_1_10_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 506,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 504,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 502,\n",
       " '0_1_10_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 499,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 499,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 498,\n",
       " '1_1_2_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 494,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 492,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 492,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 491,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 490,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 488,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 487,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 487,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 483,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 481,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 481,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 480,\n",
       " '0_2_1_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 479,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 478,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 478,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 477,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 477,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 476,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 475,\n",
       " '0_1_1_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 473,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 472,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 470,\n",
       " '1_2_2_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 470,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 470,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 466,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 466,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 466,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 465,\n",
       " '1_1_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 463,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 462,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 461,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 459,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 458,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 458,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 457,\n",
       " '2_2_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 456,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 455,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 454,\n",
       " '0_2_1_2_1_0_0_0_1_0_0_0_0_0_9_1_0_0_': 453,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 450,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 448,\n",
       " '2_2_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 448,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 448,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 448,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 447,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 444,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 444,\n",
       " '1_2_1_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 443,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 442,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 441,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 438,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 438,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 436,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 436,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 436,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 434,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 433,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 433,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 431,\n",
       " '1_2_1_2_1_0_0_0_1_0_0_0_0_0_9_1_0_0_': 429,\n",
       " '0_1_5_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 429,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 428,\n",
       " '1_1_1_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 428,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 428,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 426,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 425,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 425,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 424,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 422,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 421,\n",
       " '0_1_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 421,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 420,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 419,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 419,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 419,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 417,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 416,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 415,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 414,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 414,\n",
       " '1_1_10_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 412,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 411,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 411,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 410,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 410,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 410,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 410,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 410,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 410,\n",
       " '0_1_5_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 409,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 409,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 408,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 408,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 406,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 405,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 403,\n",
       " '1_1_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 403,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 400,\n",
       " '0_2_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 399,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 399,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 397,\n",
       " '1_1_1_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 396,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 396,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 394,\n",
       " '0_2_0_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 393,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 392,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_3_1_0_0_': 391,\n",
       " '0_2_0_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 391,\n",
       " '0_1_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 390,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 390,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 390,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 390,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 389,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 388,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 388,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 387,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 387,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 387,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 386,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 385,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 383,\n",
       " '1_1_10_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 382,\n",
       " '0_1_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 380,\n",
       " '0_1_6_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 380,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 380,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 379,\n",
       " '0_1_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 379,\n",
       " '2_2_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 378,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 378,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 376,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_3_1_0_0_': 376,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 376,\n",
       " '1_2_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 375,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 374,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 372,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 372,\n",
       " '0_2_1_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 370,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 370,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 370,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 369,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 368,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 368,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 367,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 367,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 366,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 366,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 364,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 364,\n",
       " '2_1_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 363,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 362,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 362,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 362,\n",
       " '5_1_9_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 362,\n",
       " '0_2_1_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 361,\n",
       " '2_2_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 361,\n",
       " '5_1_9_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 360,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 358,\n",
       " '0_2_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 358,\n",
       " '1_1_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 358,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 357,\n",
       " '0_2_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 357,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 357,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 356,\n",
       " '0_2_0_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 355,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 353,\n",
       " '1_2_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 353,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 352,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 352,\n",
       " '5_1_10_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 352,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 351,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 349,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 349,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 349,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 349,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 348,\n",
       " '0_2_1_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 348,\n",
       " '1_2_1_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 348,\n",
       " '3_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 348,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 348,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 347,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 347,\n",
       " '1_1_2_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 346,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 346,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 345,\n",
       " '3_1_2_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 344,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 344,\n",
       " '5_1_10_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 343,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 343,\n",
       " '0_1_6_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 342,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 342,\n",
       " '3_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 342,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 338,\n",
       " '0_1_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 338,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 336,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 335,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 335,\n",
       " '1_1_2_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 335,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 335,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_0_1_0_0_': 334,\n",
       " '1_2_1_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 333,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 333,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 331,\n",
       " '0_1_3_1_1_1_0_0_0_0_0_0_0_0_0_1_0_0_': 331,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 331,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 331,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 331,\n",
       " '1_2_2_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 330,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 330,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 329,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 328,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 327,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 327,\n",
       " '5_1_8_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 327,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 326,\n",
       " '3_1_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 325,\n",
       " '1_1_3_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 325,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 325,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 325,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 325,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 324,\n",
       " '2_1_2_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 323,\n",
       " '3_1_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 323,\n",
       " '0_1_1_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 322,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 321,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 321,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 321,\n",
       " '0_2_1_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 320,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 320,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 320,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 319,\n",
       " '1_2_0_1_0_1_0_0_0_0_0_0_0_0_7_1_0_0_': 318,\n",
       " '0_2_2_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 318,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 318,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 318,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 318,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 318,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 317,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_8_0_1_0_': 317,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 317,\n",
       " '0_2_2_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 316,\n",
       " '2_1_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 315,\n",
       " '5_1_8_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 315,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 314,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 313,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 313,\n",
       " '0_1_7_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 313,\n",
       " '1_1_5_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 312,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 311,\n",
       " '0_1_7_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 311,\n",
       " '2_1_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 311,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 311,\n",
       " '1_1_1_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 310,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 310,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 309,\n",
       " '1_2_1_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 309,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 308,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 308,\n",
       " '1_2_1_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 307,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 306,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 305,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 305,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 305,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 304,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 304,\n",
       " '1_1_3_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 303,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 303,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 303,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 302,\n",
       " '3_1_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 302,\n",
       " '0_1_5_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 302,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 302,\n",
       " '0_2_0_1_0_0_0_0_1_0_0_0_0_0_7_1_0_0_': 301,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 301,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 301,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 301,\n",
       " '1_1_3_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 300,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 300,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 300,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 299,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 299,\n",
       " '2_1_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 299,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 299,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 299,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 299,\n",
       " '5_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 298,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 296,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 296,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 296,\n",
       " '0_2_0_2_0_0_1_0_0_0_0_0_0_0_8_1_0_0_': 296,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 295,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 294,\n",
       " '0_1_1_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 294,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 293,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 293,\n",
       " '1_1_1_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 293,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 292,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 292,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 292,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 292,\n",
       " '2_1_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 291,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 291,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 291,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 290,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 290,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 290,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 289,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 289,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 289,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 289,\n",
       " '0_1_1_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 288,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 288,\n",
       " '3_1_3_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 287,\n",
       " '0_1_1_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 287,\n",
       " '2_1_9_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 287,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 287,\n",
       " '5_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 287,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 286,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 286,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 285,\n",
       " '1_1_6_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 284,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 284,\n",
       " '3_1_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 284,\n",
       " '2_1_9_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 283,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 282,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 282,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_3_1_0_0_': 280,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 280,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 280,\n",
       " '0_1_2_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 280,\n",
       " '5_1_8_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 279,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 279,\n",
       " '3_1_4_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 279,\n",
       " '3_1_4_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 279,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 278,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 278,\n",
       " '1_1_3_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 278,\n",
       " '1_2_0_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 278,\n",
       " '3_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 277,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 277,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 276,\n",
       " '3_1_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 276,\n",
       " '0_2_1_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 276,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 276,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 276,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 275,\n",
       " '5_1_4_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 275,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 275,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 275,\n",
       " '1_1_1_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 275,\n",
       " '3_2_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 274,\n",
       " '1_1_9_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 274,\n",
       " '5_1_11_1_1_0_0_0_1_0_0_0_0_0_13_0_0_1_': 274,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 274,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 274,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 274,\n",
       " '0_1_3_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 274,\n",
       " '2_2_1_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 274,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 273,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 273,\n",
       " '5_1_4_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 273,\n",
       " '5_1_9_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 273,\n",
       " '1_1_6_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 273,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 272,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 272,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 272,\n",
       " '0_1_8_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 272,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 271,\n",
       " '0_1_8_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 271,\n",
       " '2_2_1_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 271,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 270,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 270,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 269,\n",
       " '2_2_2_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 269,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 269,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 269,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 269,\n",
       " '0_2_1_2_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 268,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 268,\n",
       " '5_1_11_1_1_0_0_0_1_0_0_0_0_0_12_0_0_1_': 268,\n",
       " '0_2_0_1_7_0_0_0_1_0_0_0_0_0_7_1_0_0_': 267,\n",
       " '2_1_2_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 267,\n",
       " '5_1_4_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 266,\n",
       " '4_1_3_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 266,\n",
       " '1_1_5_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 266,\n",
       " '1_1_3_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 266,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 265,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 265,\n",
       " '1_1_1_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 265,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 265,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 265,\n",
       " '0_2_1_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 264,\n",
       " '0_1_5_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 264,\n",
       " '0_1_2_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 264,\n",
       " '0_1_3_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 263,\n",
       " '5_1_7_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 263,\n",
       " '0_1_3_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 262,\n",
       " '0_1_7_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 262,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 262,\n",
       " '1_1_6_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 261,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 261,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 260,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 260,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 260,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 260,\n",
       " '0_1_1_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 259,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 259,\n",
       " '2_1_4_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 259,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 258,\n",
       " '1_1_2_1_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 258,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 258,\n",
       " '0_2_0_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 258,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 258,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 258,\n",
       " '1_1_2_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 258,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 258,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 257,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 257,\n",
       " '0_1_5_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 257,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_5_0_0_1_': 257,\n",
       " '5_1_4_2_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 256,\n",
       " '2_2_1_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 256,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 256,\n",
       " '1_1_2_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 256,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 255,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 255,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 255,\n",
       " '2_1_2_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 255,\n",
       " '3_1_4_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 255,\n",
       " '1_2_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 254,\n",
       " '5_1_3_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 254,\n",
       " '0_1_8_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 254,\n",
       " '2_1_2_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 254,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 254,\n",
       " '5_1_5_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 253,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 253,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 253,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 252,\n",
       " '1_1_7_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 252,\n",
       " '0_1_8_1_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 252,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 252,\n",
       " '1_2_2_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 252,\n",
       " '0_1_5_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 251,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 251,\n",
       " '1_2_1_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 251,\n",
       " '2_2_2_2_1_0_1_0_0_0_0_0_0_0_10_1_0_0_': 251,\n",
       " '2_2_2_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 250,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 250,\n",
       " '2_1_2_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 250,\n",
       " '2_2_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 250,\n",
       " '0_1_6_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 250,\n",
       " '5_1_8_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 249,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 249,\n",
       " '0_1_2_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 249,\n",
       " '0_1_6_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 249,\n",
       " '3_1_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 249,\n",
       " '5_1_11_1_1_1_0_0_0_0_0_0_0_0_13_0_0_1_': 249,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 249,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 249,\n",
       " '5_1_9_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 249,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 248,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 247,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 247,\n",
       " '0_1_10_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 247,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 247,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 247,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 246,\n",
       " '3_1_2_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 246,\n",
       " '1_1_3_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 246,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 246,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 246,\n",
       " '2_1_3_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 246,\n",
       " '1_1_2_2_1_0_1_0_0_0_0_0_0_0_3_1_0_0_': 245,\n",
       " '2_2_1_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 245,\n",
       " '1_2_2_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 245,\n",
       " '0_2_1_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 245,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 245,\n",
       " '0_1_3_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 243,\n",
       " '2_2_1_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 243,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 242,\n",
       " '1_1_6_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 242,\n",
       " '0_2_0_2_0_1_0_0_0_0_0_0_0_0_8_1_0_0_': 242,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 242,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 242,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 242,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 242,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 242,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 242,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 242,\n",
       " '0_1_5_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 242,\n",
       " '1_2_1_2_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 242,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 241,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 241,\n",
       " '5_1_10_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 241,\n",
       " '1_1_2_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 241,\n",
       " '5_1_5_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 241,\n",
       " '0_1_11_1_1_1_0_0_0_0_0_0_0_0_12_0_0_1_': 241,\n",
       " '2_1_2_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 240,\n",
       " '1_1_3_1_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 240,\n",
       " '1_1_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 240,\n",
       " '1_1_8_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 240,\n",
       " '1_1_8_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 239,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 239,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 238,\n",
       " '5_1_7_2_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 238,\n",
       " '5_1_5_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 238,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 238,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 237,\n",
       " '0_1_9_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 237,\n",
       " '0_2_2_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 237,\n",
       " '0_1_6_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 237,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 237,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_4_0_0_1_': 237,\n",
       " '1_1_1_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 236,\n",
       " '5_1_7_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 236,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 236,\n",
       " '1_1_1_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 236,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 235,\n",
       " '0_1_7_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 235,\n",
       " '2_1_3_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 235,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 235,\n",
       " '0_1_2_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 235,\n",
       " '0_2_2_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 235,\n",
       " '5_1_4_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 234,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 234,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 234,\n",
       " '3_2_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 234,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 233,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 233,\n",
       " '2_2_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 233,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 233,\n",
       " '0_1_11_1_1_1_0_0_0_0_0_0_0_0_13_0_0_1_': 232,\n",
       " '1_2_0_1_7_1_0_0_0_0_0_0_0_0_7_1_0_0_': 232,\n",
       " '1_1_9_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 232,\n",
       " '0_1_7_2_1_0_1_0_0_0_0_0_0_0_13_1_0_0_': 232,\n",
       " '1_1_9_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 232,\n",
       " '1_1_3_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 232,\n",
       " '3_1_3_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 232,\n",
       " '0_2_0_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 232,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 232,\n",
       " '3_1_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 232,\n",
       " '0_2_1_2_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 232,\n",
       " '3_1_2_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 232,\n",
       " '1_1_4_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 232,\n",
       " '1_1_7_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 232,\n",
       " '0_1_5_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 232,\n",
       " '3_1_2_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 231,\n",
       " '1_1_4_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 231,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 231,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 231,\n",
       " '0_1_7_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 230,\n",
       " '1_2_2_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 230,\n",
       " '0_1_2_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 230,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 230,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 230,\n",
       " '2_2_1_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 230,\n",
       " '5_1_6_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 229,\n",
       " '5_1_3_2_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 229,\n",
       " '2_1_3_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 229,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 228,\n",
       " '3_2_1_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 228,\n",
       " '0_1_3_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 228,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 228,\n",
       " '1_1_4_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 228,\n",
       " '5_1_5_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 228,\n",
       " '5_1_4_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 228,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 228,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 228,\n",
       " '1_2_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 228,\n",
       " '2_1_5_1_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 227,\n",
       " '2_2_1_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 227,\n",
       " '0_1_8_2_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 227,\n",
       " '4_1_3_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 227,\n",
       " '2_2_1_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 227,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 226,\n",
       " '2_1_10_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 226,\n",
       " '0_1_4_1_1_1_0_0_0_0_0_0_0_0_0_1_0_0_': 226,\n",
       " '2_1_2_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 226,\n",
       " '5_1_3_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 225,\n",
       " '1_1_7_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 225,\n",
       " '0_1_4_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 225,\n",
       " '5_1_4_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 225,\n",
       " '2_1_3_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 225,\n",
       " '0_1_1_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 224,\n",
       " '3_2_1_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 224,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 224,\n",
       " '3_1_4_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 224,\n",
       " '1_1_1_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 224,\n",
       " '4_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 223,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 223,\n",
       " '0_2_1_1_1_1_0_0_0_0_0_0_0_0_0_1_0_0_': 223,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 223,\n",
       " '0_1_9_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 222,\n",
       " '1_1_2_2_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 222,\n",
       " '3_1_3_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 222,\n",
       " '3_2_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 222,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 222,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 222,\n",
       " '1_2_2_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 222,\n",
       " '0_1_2_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 222,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 221,\n",
       " '0_1_2_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 221,\n",
       " '5_1_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 221,\n",
       " '5_1_2_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 221,\n",
       " '1_2_1_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 220,\n",
       " '1_1_2_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 220,\n",
       " '5_1_8_2_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 220,\n",
       " '1_1_3_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 220,\n",
       " '1_1_6_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 220,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_7_0_1_0_': 220,\n",
       " '2_1_7_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 220,\n",
       " '5_1_7_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 220,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 220,\n",
       " '5_1_9_1_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 220,\n",
       " '3_1_7_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 219,\n",
       " '2_2_1_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 219,\n",
       " '2_1_2_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 219,\n",
       " '5_1_3_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 219,\n",
       " '1_1_4_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 218,\n",
       " '1_1_5_1_1_1_0_0_0_0_0_0_0_0_3_1_0_0_': 218,\n",
       " '0_1_3_2_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 218,\n",
       " '0_1_6_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 217,\n",
       " '5_1_4_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 217,\n",
       " '1_1_3_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 217,\n",
       " '0_1_5_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 217,\n",
       " '1_1_2_1_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 217,\n",
       " '5_1_5_2_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 217,\n",
       " '1_2_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 217,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 217,\n",
       " '2_1_4_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 216,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 216,\n",
       " '0_1_1_2_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 215,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 215,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 215,\n",
       " '0_2_1_2_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 215,\n",
       " '1_1_6_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 215,\n",
       " '5_1_5_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 215,\n",
       " '1_2_1_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 215,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 214,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_1_0_0_1_': 214,\n",
       " '0_1_1_2_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 214,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 214,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 214,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 213,\n",
       " '1_1_1_1_1_1_0_0_0_0_0_0_0_0_10_0_0_0_': 213,\n",
       " '1_1_4_1_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 213,\n",
       " '0_1_6_2_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 213,\n",
       " '0_1_2_2_1_0_1_0_0_0_0_0_0_0_5_0_0_1_': 213,\n",
       " '1_1_2_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 212,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_8_0_1_0_': 212,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_6_0_1_0_': 212,\n",
       " '0_1_10_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 212,\n",
       " '0_1_5_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 211,\n",
       " '3_1_5_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 211,\n",
       " '1_1_5_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 211,\n",
       " '5_1_10_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 211,\n",
       " '5_2_2_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 211,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 211,\n",
       " '0_1_1_2_1_0_0_1_0_0_0_0_0_0_10_1_0_0_': 210,\n",
       " '5_1_8_1_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 210,\n",
       " '0_1_6_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 210,\n",
       " '2_1_4_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 210,\n",
       " '5_1_2_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 210,\n",
       " '5_1_5_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 209,\n",
       " '1_1_3_2_1_1_0_0_0_0_0_0_0_0_4_1_0_0_': 209,\n",
       " '3_1_3_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 209,\n",
       " '1_1_8_1_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 209,\n",
       " '5_1_8_1_1_0_0_0_1_0_0_0_0_0_7_1_0_0_': 209,\n",
       " '2_1_6_1_1_1_0_0_0_0_0_0_0_0_5_1_0_0_': 209,\n",
       " '3_1_3_1_1_1_0_0_0_0_0_0_0_0_9_1_0_0_': 208,\n",
       " '5_1_8_1_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 208,\n",
       " '0_1_9_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 208,\n",
       " '0_1_3_1_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 208,\n",
       " '2_2_2_2_1_0_0_0_1_0_0_0_0_0_10_1_0_0_': 208,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 208,\n",
       " '3_1_4_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 208,\n",
       " '2_1_10_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 207,\n",
       " '2_1_8_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 207,\n",
       " '5_1_6_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 207,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_7_0_1_0_': 207,\n",
       " '5_1_7_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 207,\n",
       " '2_1_3_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 207,\n",
       " '0_1_1_1_5_1_0_0_0_0_0_0_0_0_7_1_0_0_': 207,\n",
       " '0_1_0_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 207,\n",
       " '5_1_3_2_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 206,\n",
       " '0_1_1_2_1_0_0_0_1_0_0_0_0_0_9_1_0_0_': 206,\n",
       " '1_1_2_1_1_1_0_0_0_0_0_0_0_0_6_0_1_0_': 206,\n",
       " '5_1_3_2_1_0_0_1_0_0_0_0_0_0_11_1_0_0_': 206,\n",
       " '1_1_3_1_1_0_0_1_0_0_0_0_0_0_8_1_0_0_': 206,\n",
       " '1_1_4_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 206,\n",
       " '1_1_7_1_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 205,\n",
       " '1_2_1_2_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 204,\n",
       " '2_1_3_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 204,\n",
       " '2_1_4_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 204,\n",
       " '0_1_1_2_1_0_1_0_0_0_0_0_0_0_10_0_0_0_': 204,\n",
       " '3_1_5_1_1_1_0_0_0_0_0_0_0_0_6_1_0_0_': 204,\n",
       " '0_1_10_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 204,\n",
       " '1_1_6_1_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 204,\n",
       " '5_1_3_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 204,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 203,\n",
       " '0_2_2_1_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 203,\n",
       " '3_1_3_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 203,\n",
       " '0_2_3_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 203,\n",
       " '0_1_6_1_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 203,\n",
       " '1_1_4_1_1_1_0_0_0_0_0_0_0_0_1_0_0_1_': 203,\n",
       " '0_1_4_2_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 203,\n",
       " '0_2_1_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 203,\n",
       " '0_1_7_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 202,\n",
       " '0_1_2_2_1_1_0_0_0_0_0_0_0_0_2_0_0_1_': 202,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 202,\n",
       " '1_1_2_2_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 202,\n",
       " '5_1_8_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 202,\n",
       " '0_1_7_2_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 202,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_0_0_0_1_': 201,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_8_0_1_0_': 201,\n",
       " '5_1_8_1_1_1_0_0_0_0_0_0_0_0_7_1_0_0_': 200,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_4_0_0_1_': 200,\n",
       " '0_1_3_2_1_0_1_0_0_0_0_0_0_0_5_0_0_1_': 200,\n",
       " '5_1_7_1_1_0_0_1_0_0_0_0_0_0_13_1_0_0_': 200,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_0_0_0_1_': 200,\n",
       " '3_1_9_1_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 200,\n",
       " '5_1_7_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 200,\n",
       " '5_1_3_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 200,\n",
       " '0_1_7_2_1_0_1_0_0_0_0_0_0_0_11_1_0_0_': 199,\n",
       " '2_1_5_2_1_0_1_0_0_0_0_0_0_0_6_1_0_0_': 199,\n",
       " '5_1_3_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 199,\n",
       " '0_2_2_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 199,\n",
       " '5_1_4_2_1_0_0_1_0_0_0_0_0_0_12_1_0_0_': 199,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 198,\n",
       " '1_1_8_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 198,\n",
       " '1_2_0_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 198,\n",
       " '0_1_2_1_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 198,\n",
       " '1_2_1_1_1_1_0_0_0_0_0_0_0_0_7_0_1_0_': 198,\n",
       " '1_1_3_2_1_0_1_0_0_0_0_0_0_0_3_1_0_0_': 198,\n",
       " '0_1_6_2_1_0_1_0_0_0_0_0_0_0_3_0_0_1_': 198,\n",
       " '1_2_3_1_1_1_0_0_0_0_0_0_0_0_11_1_0_0_': 198,\n",
       " '2_1_4_1_1_1_0_0_0_0_0_0_0_0_10_1_0_0_': 197,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 197,\n",
       " '5_1_4_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 197,\n",
       " '1_1_6_2_1_0_1_0_0_0_0_0_0_0_7_1_0_0_': 197,\n",
       " '1_1_6_1_1_1_0_0_0_0_0_0_0_0_3_0_0_1_': 196,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_2_1_0_0_': 196,\n",
       " '0_2_0_2_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 196,\n",
       " '1_1_3_1_1_1_0_0_0_0_0_0_0_0_11_0_1_0_': 196,\n",
       " '1_1_5_1_1_0_1_0_0_0_0_0_0_0_8_1_0_0_': 195,\n",
       " '1_1_3_2_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 195,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 195,\n",
       " '1_1_7_1_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 195,\n",
       " '0_2_2_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 195,\n",
       " '0_2_0_1_7_1_0_0_0_0_0_0_0_0_8_1_0_0_': 195,\n",
       " '1_1_3_2_1_0_0_0_1_0_0_0_0_0_12_1_0_0_': 194,\n",
       " '1_2_2_1_1_0_0_0_1_0_0_0_0_0_6_1_0_0_': 194,\n",
       " '1_1_7_2_1_0_1_0_0_0_0_0_0_0_13_1_0_0_': 194,\n",
       " '0_2_1_2_1_0_0_1_0_0_0_0_0_0_9_1_0_0_': 194,\n",
       " '0_1_3_2_1_0_0_0_1_0_0_0_0_0_11_1_0_0_': 194,\n",
       " '0_1_3_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 194,\n",
       " '5_1_6_1_1_1_0_0_0_0_0_0_0_0_8_1_0_0_': 194,\n",
       " '1_2_1_2_1_0_1_0_0_0_0_0_0_0_5_1_0_0_': 194,\n",
       " '3_1_8_1_1_1_0_0_0_0_0_0_0_0_12_1_0_0_': 193,\n",
       " '1_1_4_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 193,\n",
       " '5_1_4_2_1_0_1_0_0_0_0_0_0_0_12_1_0_0_': 193,\n",
       " '2_2_1_1_1_0_0_1_0_0_0_0_0_0_7_1_0_0_': 193,\n",
       " '0_1_4_1_1_0_1_0_0_0_0_0_0_0_2_0_0_1_': 193,\n",
       " '1_1_8_1_1_0_0_0_1_0_0_0_0_0_13_1_0_0_': 193,\n",
       " '1_1_5_2_1_0_1_0_0_0_0_0_0_0_4_1_0_0_': 193,\n",
       " '5_1_4_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 193,\n",
       " '2_1_4_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 193,\n",
       " '5_1_3_1_1_0_0_1_0_0_0_0_0_0_6_1_0_0_': 192,\n",
       " '0_1_9_2_1_1_0_0_0_0_0_0_0_0_13_1_0_0_': 192,\n",
       " '0_1_4_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 192,\n",
       " '0_1_5_2_1_0_1_0_0_0_0_0_0_0_9_1_0_0_': 192,\n",
       " '2_1_2_1_1_1_0_0_0_0_0_0_0_0_8_0_1_0_': 192,\n",
       " '2_1_3_1_1_0_0_0_1_0_0_0_0_0_8_1_0_0_': 192,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 'new_ind'\n",
    "pd.concat([train[c], test[c]]).value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "\n",
    "for c in cat_features + ['new_ind']:\n",
    "    d = pd.concat([train[c], test[c]]).value_counts().to_dict()\n",
    "    train['%s_count'%c] = train[c].apply(lambda x: d.get(x, 0))\n",
    "    test['%s_count'%c] = test[c].apply(lambda x: d.get(x, 0))\n",
    "    cat_count_features.append('%s_count'%c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 Features\n",
    "\n",
    "1) num + bin features\n",
    "\n",
    "2) missing 개수\n",
    "\n",
    "3) category count features\n",
    "\n",
    "4) new_ind 조합 category count features\n",
    "\n",
    "5) one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [train[num_features+cat_count_features].values,X_cat,]\n",
    "test_list = [test[num_features+cat_count_features].values,X_t_cat,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ssp.hstack(train_list).tocsr()\n",
    "X_test = ssp.hstack(test_list).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_leaves = 15\n",
    "min_data_in_leaf = 2000\n",
    "feature_fraction = 0.6\n",
    "num_boost_round = 10000\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"num_leaves\": num_leaves,\n",
    "           \"max_bin\": 256,\n",
    "          \"feature_fraction\": feature_fraction,\n",
    "          \"verbosity\": 0,\n",
    "          \"drop_rate\": 0.1,\n",
    "          \"is_unbalance\": False,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_child_weight\": 150,\n",
    "          \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.9\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed 별 16번 cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.293401\n",
      "[200]\tvalid_0's binary_logloss: 0.151285\tvalid_0's gini: 0.296137\n",
      "[300]\tvalid_0's binary_logloss: 0.151296\tvalid_0's gini: 0.295778\n",
      "Early stopping, best iteration is:\n",
      "[235]\tvalid_0's binary_logloss: 0.151265\tvalid_0's gini: 0.296665\n",
      "0.29666456778791145\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15216\tvalid_0's gini: 0.271447\n",
      "[200]\tvalid_0's binary_logloss: 0.152144\tvalid_0's gini: 0.27349\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's binary_logloss: 0.152127\tvalid_0's gini: 0.272928\n",
      "0.2729277334928809\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152081\tvalid_0's gini: 0.279382\n",
      "[200]\tvalid_0's binary_logloss: 0.152048\tvalid_0's gini: 0.28106\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's binary_logloss: 0.152014\tvalid_0's gini: 0.281321\n",
      "0.28132063784139355\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151849\tvalid_0's gini: 0.279388\n",
      "[200]\tvalid_0's binary_logloss: 0.151786\tvalid_0's gini: 0.282396\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.282331\n",
      "0.2823311851029831\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.295644\n",
      "[200]\tvalid_0's binary_logloss: 0.151404\tvalid_0's gini: 0.295798\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's binary_logloss: 0.151368\tvalid_0's gini: 0.296833\n",
      "0.2968333713391588\n",
      "cv score:\n",
      "0.28597214631799395\n",
      "current score: 0.28597214631799395 1\n",
      "[0.29666456778791145, 0.2729277334928809, 0.28132063784139355, 0.2823311851029831, 0.2968333713391588]\n",
      "[235, 145, 129, 174, 148] 166.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151471\tvalid_0's gini: 0.292196\n",
      "[200]\tvalid_0's binary_logloss: 0.15141\tvalid_0's gini: 0.293683\n",
      "[300]\tvalid_0's binary_logloss: 0.151389\tvalid_0's gini: 0.294652\n",
      "Early stopping, best iteration is:\n",
      "[292]\tvalid_0's binary_logloss: 0.151382\tvalid_0's gini: 0.294693\n",
      "0.29469324629541976\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152178\tvalid_0's gini: 0.272417\n",
      "[200]\tvalid_0's binary_logloss: 0.152146\tvalid_0's gini: 0.27454\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's binary_logloss: 0.152132\tvalid_0's gini: 0.274858\n",
      "0.274858404044378\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152029\tvalid_0's gini: 0.281289\n",
      "[200]\tvalid_0's binary_logloss: 0.151975\tvalid_0's gini: 0.283232\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.151954\tvalid_0's gini: 0.283787\n",
      "0.2837870722034973\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151784\tvalid_0's gini: 0.281077\n",
      "[200]\tvalid_0's binary_logloss: 0.151769\tvalid_0's gini: 0.281953\n",
      "[300]\tvalid_0's binary_logloss: 0.151825\tvalid_0's gini: 0.281328\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid_0's binary_logloss: 0.151748\tvalid_0's gini: 0.283076\n",
      "0.28307560290826894\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151493\tvalid_0's gini: 0.292969\n",
      "[200]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.293871\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's binary_logloss: 0.151442\tvalid_0's gini: 0.294379\n",
      "0.29437870266645344\n",
      "cv score:\n",
      "0.2860831650576075\n",
      "current score: 0.2875325709149362 2\n",
      "[0.29469324629541976, 0.274858404044378, 0.2837870722034973, 0.28307560290826894, 0.29437870266645344]\n",
      "[292, 196, 174, 252, 167] 216.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15144\tvalid_0's gini: 0.294292\n",
      "[200]\tvalid_0's binary_logloss: 0.151312\tvalid_0's gini: 0.298304\n",
      "[300]\tvalid_0's binary_logloss: 0.151349\tvalid_0's gini: 0.297353\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's binary_logloss: 0.151294\tvalid_0's gini: 0.298788\n",
      "0.29878766725997635\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152214\tvalid_0's gini: 0.271424\n",
      "[200]\tvalid_0's binary_logloss: 0.152136\tvalid_0's gini: 0.275273\n",
      "[300]\tvalid_0's binary_logloss: 0.152174\tvalid_0's gini: 0.274428\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.152136\tvalid_0's gini: 0.275273\n",
      "0.2752734969270364\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151958\tvalid_0's gini: 0.282889\n",
      "[200]\tvalid_0's binary_logloss: 0.151877\tvalid_0's gini: 0.285479\n",
      "Early stopping, best iteration is:\n",
      "[198]\tvalid_0's binary_logloss: 0.151873\tvalid_0's gini: 0.285678\n",
      "0.2856775539391694\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151774\tvalid_0's gini: 0.281243\n",
      "[200]\tvalid_0's binary_logloss: 0.151669\tvalid_0's gini: 0.284941\n",
      "[300]\tvalid_0's binary_logloss: 0.15171\tvalid_0's gini: 0.284324\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid_0's binary_logloss: 0.151648\tvalid_0's gini: 0.285622\n",
      "0.2856215369582491\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151478\tvalid_0's gini: 0.294346\n",
      "[200]\tvalid_0's binary_logloss: 0.151468\tvalid_0's gini: 0.295475\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's binary_logloss: 0.151433\tvalid_0's gini: 0.296139\n",
      "0.2961389003412815\n",
      "cv score:\n",
      "0.2882027439866094\n",
      "current score: 0.2887395531775446 3\n",
      "[0.29878766725997635, 0.2752734969270364, 0.2856775539391694, 0.2856215369582491, 0.2961389003412815]\n",
      "[232, 200, 198, 230, 150] 202.0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151443\tvalid_0's gini: 0.293574\n",
      "[200]\tvalid_0's binary_logloss: 0.151346\tvalid_0's gini: 0.295883\n",
      "[300]\tvalid_0's binary_logloss: 0.151361\tvalid_0's gini: 0.296137\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.151322\tvalid_0's gini: 0.297066\n",
      "0.29706590541443856\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152137\tvalid_0's gini: 0.272652\n",
      "[200]\tvalid_0's binary_logloss: 0.152117\tvalid_0's gini: 0.273089\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.152091\tvalid_0's gini: 0.273622\n",
      "0.2736221991236435\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152008\tvalid_0's gini: 0.281849\n",
      "[200]\tvalid_0's binary_logloss: 0.152029\tvalid_0's gini: 0.282527\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's binary_logloss: 0.151952\tvalid_0's gini: 0.284661\n",
      "0.2846607740858593\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151787\tvalid_0's gini: 0.28141\n",
      "[200]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.283579\n",
      "[300]\tvalid_0's binary_logloss: 0.151768\tvalid_0's gini: 0.283111\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's binary_logloss: 0.151727\tvalid_0's gini: 0.284009\n",
      "0.28400898735202\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151494\tvalid_0's gini: 0.293422\n",
      "[200]\tvalid_0's binary_logloss: 0.151499\tvalid_0's gini: 0.293971\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.15147\tvalid_0's gini: 0.294281\n",
      "0.29428125131649263\n",
      "cv score:\n",
      "0.2866631705466922\n",
      "current score: 0.2889115773930896 4\n",
      "[0.29706590541443856, 0.2736221991236435, 0.2846607740858593, 0.28400898735202, 0.29428125131649263]\n",
      "[254, 125, 162, 220, 125] 177.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151449\tvalid_0's gini: 0.292657\n",
      "[200]\tvalid_0's binary_logloss: 0.151309\tvalid_0's gini: 0.296175\n",
      "[300]\tvalid_0's binary_logloss: 0.151309\tvalid_0's gini: 0.296707\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid_0's binary_logloss: 0.151283\tvalid_0's gini: 0.297437\n",
      "0.29743662222538836\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152196\tvalid_0's gini: 0.272397\n",
      "[200]\tvalid_0's binary_logloss: 0.152154\tvalid_0's gini: 0.274593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's binary_logloss: 0.152184\tvalid_0's gini: 0.274817\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's binary_logloss: 0.152146\tvalid_0's gini: 0.274895\n",
      "0.27489529368834104\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15207\tvalid_0's gini: 0.279475\n",
      "[200]\tvalid_0's binary_logloss: 0.152035\tvalid_0's gini: 0.281024\n",
      "[300]\tvalid_0's binary_logloss: 0.152056\tvalid_0's gini: 0.281188\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid_0's binary_logloss: 0.152017\tvalid_0's gini: 0.28151\n",
      "0.28150996851408605\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151784\tvalid_0's gini: 0.280768\n",
      "[200]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.282347\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's binary_logloss: 0.151718\tvalid_0's gini: 0.282987\n",
      "0.28298713530564457\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151477\tvalid_0's gini: 0.293967\n",
      "[200]\tvalid_0's binary_logloss: 0.151441\tvalid_0's gini: 0.295797\n",
      "Early stopping, best iteration is:\n",
      "[192]\tvalid_0's binary_logloss: 0.151436\tvalid_0's gini: 0.296111\n",
      "0.29611137118904884\n",
      "cv score:\n",
      "0.28652316925499893\n",
      "current score: 0.2890815697645405 5\n",
      "[0.29743662222538836, 0.27489529368834104, 0.28150996851408605, 0.28298713530564457, 0.29611137118904884]\n",
      "[242, 205, 270, 147, 192] 211.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151462\tvalid_0's gini: 0.291961\n",
      "[200]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.295051\n",
      "[300]\tvalid_0's binary_logloss: 0.151329\tvalid_0's gini: 0.296135\n",
      "[400]\tvalid_0's binary_logloss: 0.151338\tvalid_0's gini: 0.29682\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's binary_logloss: 0.151312\tvalid_0's gini: 0.297362\n",
      "0.29736152889458917\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152119\tvalid_0's gini: 0.273374\n",
      "[200]\tvalid_0's binary_logloss: 0.152122\tvalid_0's gini: 0.274575\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's binary_logloss: 0.152094\tvalid_0's gini: 0.274552\n",
      "0.2745523847571541\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15204\tvalid_0's gini: 0.280672\n",
      "[200]\tvalid_0's binary_logloss: 0.151956\tvalid_0's gini: 0.283306\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid_0's binary_logloss: 0.151937\tvalid_0's gini: 0.283813\n",
      "0.2838133329685625\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151721\tvalid_0's gini: 0.283882\n",
      "[200]\tvalid_0's binary_logloss: 0.151666\tvalid_0's gini: 0.285833\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's binary_logloss: 0.151654\tvalid_0's gini: 0.285649\n",
      "0.2856485735256103\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15146\tvalid_0's gini: 0.294891\n",
      "[200]\tvalid_0's binary_logloss: 0.151493\tvalid_0's gini: 0.294389\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.151437\tvalid_0's gini: 0.295662\n",
      "0.295661760524777\n",
      "cv score:\n",
      "0.2873242056245988\n",
      "current score: 0.2893071481821934 6\n",
      "[0.29736152889458917, 0.2745523847571541, 0.2838133329685625, 0.2856485735256103, 0.295661760524777]\n",
      "[344, 158, 159, 166, 120] 189.4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151494\tvalid_0's gini: 0.291692\n",
      "[200]\tvalid_0's binary_logloss: 0.151356\tvalid_0's gini: 0.295657\n",
      "[300]\tvalid_0's binary_logloss: 0.151366\tvalid_0's gini: 0.295829\n",
      "Early stopping, best iteration is:\n",
      "[225]\tvalid_0's binary_logloss: 0.151331\tvalid_0's gini: 0.29669\n",
      "0.29669030213213143\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152174\tvalid_0's gini: 0.272569\n",
      "[200]\tvalid_0's binary_logloss: 0.15216\tvalid_0's gini: 0.273503\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's binary_logloss: 0.152126\tvalid_0's gini: 0.274139\n",
      "0.27413868226848387\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152009\tvalid_0's gini: 0.282061\n",
      "[200]\tvalid_0's binary_logloss: 0.151987\tvalid_0's gini: 0.283875\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's binary_logloss: 0.151945\tvalid_0's gini: 0.284576\n",
      "0.2845758917384268\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151755\tvalid_0's gini: 0.282189\n",
      "[200]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.283132\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.283386\n",
      "0.2833857719706613\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151491\tvalid_0's gini: 0.293719\n",
      "[200]\tvalid_0's binary_logloss: 0.151498\tvalid_0's gini: 0.294603\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's binary_logloss: 0.151469\tvalid_0's gini: 0.294327\n",
      "0.29432702069252903\n",
      "cv score:\n",
      "0.2865222277588866\n",
      "current score: 0.2893046408480021 7\n",
      "[0.29669030213213143, 0.27413868226848387, 0.2845758917384268, 0.2833857719706613, 0.29432702069252903]\n",
      "[225, 154, 161, 153, 106] 159.8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151446\tvalid_0's gini: 0.291975\n",
      "[200]\tvalid_0's binary_logloss: 0.151317\tvalid_0's gini: 0.294938\n",
      "[300]\tvalid_0's binary_logloss: 0.151352\tvalid_0's gini: 0.294082\n",
      "Early stopping, best iteration is:\n",
      "[208]\tvalid_0's binary_logloss: 0.151308\tvalid_0's gini: 0.29524\n",
      "0.2952402417568844\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152191\tvalid_0's gini: 0.27203\n",
      "[200]\tvalid_0's binary_logloss: 0.152172\tvalid_0's gini: 0.27325\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's binary_logloss: 0.152155\tvalid_0's gini: 0.273674\n",
      "0.27367423039924627\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152026\tvalid_0's gini: 0.280562\n",
      "[200]\tvalid_0's binary_logloss: 0.151958\tvalid_0's gini: 0.283393\n",
      "[300]\tvalid_0's binary_logloss: 0.151996\tvalid_0's gini: 0.283236\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's binary_logloss: 0.151939\tvalid_0's gini: 0.283882\n",
      "0.28388207710138114\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151743\tvalid_0's gini: 0.283142\n",
      "[200]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.285138\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's binary_logloss: 0.151678\tvalid_0's gini: 0.285491\n",
      "0.2854910839133253\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151483\tvalid_0's gini: 0.294074\n",
      "[200]\tvalid_0's binary_logloss: 0.151431\tvalid_0's gini: 0.295011\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.295351\n",
      "0.2953507683264196\n",
      "cv score:\n",
      "0.28660121794849813\n",
      "current score: 0.28931681262290665 8\n",
      "[0.2952402417568844, 0.27367423039924627, 0.28388207710138114, 0.2854910839133253, 0.2953507683264196]\n",
      "[208, 178, 221, 191, 138] 187.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151442\tvalid_0's gini: 0.293363\n",
      "[200]\tvalid_0's binary_logloss: 0.151327\tvalid_0's gini: 0.296344\n",
      "[300]\tvalid_0's binary_logloss: 0.151364\tvalid_0's gini: 0.295479\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's binary_logloss: 0.151314\tvalid_0's gini: 0.297022\n",
      "0.2970224214452311\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152091\tvalid_0's gini: 0.274874\n",
      "[200]\tvalid_0's binary_logloss: 0.152084\tvalid_0's gini: 0.276645\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's binary_logloss: 0.152062\tvalid_0's gini: 0.275719\n",
      "0.2757185596315549\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152016\tvalid_0's gini: 0.280747\n",
      "[200]\tvalid_0's binary_logloss: 0.151963\tvalid_0's gini: 0.283082\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's binary_logloss: 0.151959\tvalid_0's gini: 0.282969\n",
      "0.2829690945796141\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151808\tvalid_0's gini: 0.281272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.28439\n",
      "[300]\tvalid_0's binary_logloss: 0.151781\tvalid_0's gini: 0.284739\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's binary_logloss: 0.151729\tvalid_0's gini: 0.284521\n",
      "0.2845214444293345\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151476\tvalid_0's gini: 0.294332\n",
      "[200]\tvalid_0's binary_logloss: 0.151476\tvalid_0's gini: 0.294753\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's binary_logloss: 0.15143\tvalid_0's gini: 0.295816\n",
      "0.29581611108323624\n",
      "cv score:\n",
      "0.2871546368219128\n",
      "current score: 0.2893751595015699 9\n",
      "[0.2970224214452311, 0.2757185596315549, 0.2829690945796141, 0.2845214444293345, 0.29581611108323624]\n",
      "[217, 107, 178, 201, 155] 171.6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151472\tvalid_0's gini: 0.292536\n",
      "[200]\tvalid_0's binary_logloss: 0.151406\tvalid_0's gini: 0.293324\n",
      "[300]\tvalid_0's binary_logloss: 0.151443\tvalid_0's gini: 0.293296\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's binary_logloss: 0.151391\tvalid_0's gini: 0.293853\n",
      "0.2938530143307667\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152139\tvalid_0's gini: 0.27328\n",
      "[200]\tvalid_0's binary_logloss: 0.152106\tvalid_0's gini: 0.27557\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's binary_logloss: 0.152092\tvalid_0's gini: 0.275635\n",
      "0.2756350315117929\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152004\tvalid_0's gini: 0.281785\n",
      "[200]\tvalid_0's binary_logloss: 0.152024\tvalid_0's gini: 0.281645\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.151974\tvalid_0's gini: 0.283011\n",
      "0.28301136496794166\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151772\tvalid_0's gini: 0.282538\n",
      "[200]\tvalid_0's binary_logloss: 0.151766\tvalid_0's gini: 0.282832\n",
      "[300]\tvalid_0's binary_logloss: 0.15176\tvalid_0's gini: 0.28448\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid_0's binary_logloss: 0.151722\tvalid_0's gini: 0.284911\n",
      "0.2849108746770171\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151573\tvalid_0's gini: 0.290475\n",
      "[200]\tvalid_0's binary_logloss: 0.151565\tvalid_0's gini: 0.291588\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's binary_logloss: 0.151517\tvalid_0's gini: 0.292301\n",
      "0.2923005671990124\n",
      "cv score:\n",
      "0.2858447575699159\n",
      "current score: 0.289317773569425 10\n",
      "[0.2938530143307667, 0.2756350315117929, 0.28301136496794166, 0.2849108746770171, 0.2923005671990124]\n",
      "[213, 191, 137, 261, 129] 186.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151454\tvalid_0's gini: 0.293688\n",
      "[200]\tvalid_0's binary_logloss: 0.151319\tvalid_0's gini: 0.297307\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's binary_logloss: 0.151314\tvalid_0's gini: 0.297301\n",
      "0.29730124366161176\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15213\tvalid_0's gini: 0.274282\n",
      "[200]\tvalid_0's binary_logloss: 0.152163\tvalid_0's gini: 0.273949\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.152125\tvalid_0's gini: 0.274515\n",
      "0.27451531428160303\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152021\tvalid_0's gini: 0.28023\n",
      "[200]\tvalid_0's binary_logloss: 0.151976\tvalid_0's gini: 0.281458\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid_0's binary_logloss: 0.151963\tvalid_0's gini: 0.282216\n",
      "0.2822156296293891\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15176\tvalid_0's gini: 0.282808\n",
      "[200]\tvalid_0's binary_logloss: 0.151681\tvalid_0's gini: 0.285933\n",
      "[300]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.285193\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's binary_logloss: 0.151666\tvalid_0's gini: 0.286507\n",
      "0.28650715331694926\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151502\tvalid_0's gini: 0.2935\n",
      "[200]\tvalid_0's binary_logloss: 0.151455\tvalid_0's gini: 0.294914\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's binary_logloss: 0.15144\tvalid_0's gini: 0.29596\n",
      "0.29595975385055123\n",
      "cv score:\n",
      "0.28725549746628665\n",
      "current score: 0.2893777528354913 11\n",
      "[0.29730124366161176, 0.27451531428160303, 0.2822156296293891, 0.28650715331694926, 0.29595975385055123]\n",
      "[195, 108, 142, 209, 127] 156.2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151407\tvalid_0's gini: 0.294033\n",
      "[200]\tvalid_0's binary_logloss: 0.151352\tvalid_0's gini: 0.294705\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's binary_logloss: 0.151336\tvalid_0's gini: 0.295343\n",
      "0.2953432514663995\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152209\tvalid_0's gini: 0.271002\n",
      "[200]\tvalid_0's binary_logloss: 0.152136\tvalid_0's gini: 0.274577\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's binary_logloss: 0.152118\tvalid_0's gini: 0.274712\n",
      "0.2747119023286961\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15204\tvalid_0's gini: 0.28027\n",
      "[200]\tvalid_0's binary_logloss: 0.15199\tvalid_0's gini: 0.281738\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's binary_logloss: 0.15198\tvalid_0's gini: 0.281989\n",
      "0.28198900344628036\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151775\tvalid_0's gini: 0.28241\n",
      "[200]\tvalid_0's binary_logloss: 0.151734\tvalid_0's gini: 0.283925\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's binary_logloss: 0.151721\tvalid_0's gini: 0.284323\n",
      "0.28432270878146754\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151485\tvalid_0's gini: 0.29348\n",
      "[200]\tvalid_0's binary_logloss: 0.151483\tvalid_0's gini: 0.293925\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.151449\tvalid_0's gini: 0.294785\n",
      "0.29478518472750487\n",
      "cv score:\n",
      "0.2861239009597362\n",
      "current score: 0.2893238816399851 12\n",
      "[0.2953432514663995, 0.2747119023286961, 0.28198900344628036, 0.28432270878146754, 0.29478518472750487]\n",
      "[171, 166, 171, 191, 125] 164.8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151491\tvalid_0's gini: 0.292589\n",
      "[200]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.296125\n",
      "[300]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.296414\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's binary_logloss: 0.151335\tvalid_0's gini: 0.296902\n",
      "0.2969022729196483\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152096\tvalid_0's gini: 0.2741\n",
      "[200]\tvalid_0's binary_logloss: 0.152102\tvalid_0's gini: 0.274993\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid_0's binary_logloss: 0.152074\tvalid_0's gini: 0.274983\n",
      "0.2749832461542907\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151987\tvalid_0's gini: 0.282115\n",
      "[200]\tvalid_0's binary_logloss: 0.15196\tvalid_0's gini: 0.282761\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.15194\tvalid_0's gini: 0.283134\n",
      "0.28313422596734533\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151784\tvalid_0's gini: 0.282368\n",
      "[200]\tvalid_0's binary_logloss: 0.15172\tvalid_0's gini: 0.284048\n",
      "[300]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.284533\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.284699\n",
      "0.2846994606150561\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151435\tvalid_0's gini: 0.294397\n",
      "[200]\tvalid_0's binary_logloss: 0.151398\tvalid_0's gini: 0.296372\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's binary_logloss: 0.151354\tvalid_0's gini: 0.297332\n",
      "0.29733206339895024\n",
      "cv score:\n",
      "0.28738246602129036\n",
      "current score: 0.2893985282806045 13\n",
      "[0.2969022729196483, 0.2749832461542907, 0.28313422596734533, 0.2846994606150561, 0.29733206339895024]\n",
      "[222, 104, 130, 204, 151] 162.2\n",
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.151422\tvalid_0's gini: 0.294291\n",
      "[200]\tvalid_0's binary_logloss: 0.15135\tvalid_0's gini: 0.294928\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's binary_logloss: 0.151366\tvalid_0's gini: 0.295136\n",
      "0.2951355322304417\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152162\tvalid_0's gini: 0.272096\n",
      "[200]\tvalid_0's binary_logloss: 0.152211\tvalid_0's gini: 0.272337\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.152155\tvalid_0's gini: 0.272699\n",
      "0.2726994717885202\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151981\tvalid_0's gini: 0.282198\n",
      "[200]\tvalid_0's binary_logloss: 0.151905\tvalid_0's gini: 0.284303\n",
      "[300]\tvalid_0's binary_logloss: 0.151991\tvalid_0's gini: 0.281771\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's binary_logloss: 0.151883\tvalid_0's gini: 0.285035\n",
      "0.28503507136225276\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151777\tvalid_0's gini: 0.281449\n",
      "[200]\tvalid_0's binary_logloss: 0.151709\tvalid_0's gini: 0.283572\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's binary_logloss: 0.151675\tvalid_0's gini: 0.284336\n",
      "0.2843364480834604\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151472\tvalid_0's gini: 0.294852\n",
      "[200]\tvalid_0's binary_logloss: 0.151394\tvalid_0's gini: 0.297039\n",
      "[300]\tvalid_0's binary_logloss: 0.151431\tvalid_0's gini: 0.296441\n",
      "Early stopping, best iteration is:\n",
      "[235]\tvalid_0's binary_logloss: 0.151374\tvalid_0's gini: 0.297954\n",
      "0.29795410808728157\n",
      "cv score:\n",
      "0.28700186400766353\n",
      "current score: 0.2894746779071634 14\n",
      "[0.2951355322304417, 0.2726994717885202, 0.28503507136225276, 0.2843364480834604, 0.29795410808728157]\n",
      "[135, 120, 209, 171, 235] 174.0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151406\tvalid_0's gini: 0.294203\n",
      "[200]\tvalid_0's binary_logloss: 0.151286\tvalid_0's gini: 0.297694\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid_0's binary_logloss: 0.151269\tvalid_0's gini: 0.298159\n",
      "0.2981587671445625\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152207\tvalid_0's gini: 0.271754\n",
      "[200]\tvalid_0's binary_logloss: 0.152203\tvalid_0's gini: 0.271969\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.152171\tvalid_0's gini: 0.272785\n",
      "0.2727852341830146\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152055\tvalid_0's gini: 0.280693\n",
      "[200]\tvalid_0's binary_logloss: 0.152015\tvalid_0's gini: 0.282385\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's binary_logloss: 0.151998\tvalid_0's gini: 0.282162\n",
      "0.28216247317946075\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151779\tvalid_0's gini: 0.282026\n",
      "[200]\tvalid_0's binary_logloss: 0.151717\tvalid_0's gini: 0.283863\n",
      "Early stopping, best iteration is:\n",
      "[189]\tvalid_0's binary_logloss: 0.15171\tvalid_0's gini: 0.283861\n",
      "0.28386135113794697\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.15144\tvalid_0's gini: 0.294904\n",
      "[200]\tvalid_0's binary_logloss: 0.151391\tvalid_0's gini: 0.296492\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's binary_logloss: 0.151381\tvalid_0's gini: 0.29673\n",
      "0.29673011993566145\n",
      "cv score:\n",
      "0.2866884248899059\n",
      "current score: 0.2894728934529478 15\n",
      "[0.2981587671445625, 0.2727852341830146, 0.28216247317946075, 0.28386135113794697, 0.29673011993566145]\n",
      "[188, 174, 146, 189, 183] 176.0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.292981\n",
      "[200]\tvalid_0's binary_logloss: 0.151331\tvalid_0's gini: 0.295645\n",
      "[300]\tvalid_0's binary_logloss: 0.151333\tvalid_0's gini: 0.296095\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's binary_logloss: 0.151307\tvalid_0's gini: 0.29634\n",
      "0.2963398585514422\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152173\tvalid_0's gini: 0.271318\n",
      "[200]\tvalid_0's binary_logloss: 0.152152\tvalid_0's gini: 0.273529\n",
      "[300]\tvalid_0's binary_logloss: 0.152224\tvalid_0's gini: 0.272702\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's binary_logloss: 0.152145\tvalid_0's gini: 0.274001\n",
      "0.2740005550645688\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.152043\tvalid_0's gini: 0.280483\n",
      "[200]\tvalid_0's binary_logloss: 0.15193\tvalid_0's gini: 0.284354\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid_0's binary_logloss: 0.151924\tvalid_0's gini: 0.284523\n",
      "0.28452262678954554\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151723\tvalid_0's gini: 0.283939\n",
      "[200]\tvalid_0's binary_logloss: 0.151636\tvalid_0's gini: 0.286112\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's binary_logloss: 0.151615\tvalid_0's gini: 0.28674\n",
      "0.28673960430372236\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.151424\tvalid_0's gini: 0.295103\n",
      "[200]\tvalid_0's binary_logloss: 0.151387\tvalid_0's gini: 0.296268\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid_0's binary_logloss: 0.151363\tvalid_0's gini: 0.297121\n",
      "0.29712134419822034\n",
      "cv score:\n",
      "0.2876350488364803\n",
      "current score: 0.28953770365339326 16\n",
      "[0.2963398585514422, 0.2740005550645688, 0.28452262678954554, 0.28673960430372236, 0.29712134419822034]\n",
      "[247, 213, 199, 181, 153] 198.6\n",
      "[0.28597214631799395, 0.2860831650576075, 0.2882027439866094, 0.2866631705466922, 0.28652316925499893, 0.2873242056245988, 0.2865222277588866, 0.28660121794849813, 0.2871546368219128, 0.2858447575699159, 0.28725549746628665, 0.2861239009597362, 0.28738246602129036, 0.28700186400766353, 0.2866884248899059, 0.2876350488364803]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/lgbm3_cv_avg.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8d54b116f9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_cv_pred\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m16.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lgbm3_pred_avg.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_cv_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m16.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lgbm3_cv_avg.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/base-workspace/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/base-workspace/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/base-workspace/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/lgbm3_cv_avg.csv'"
     ]
    }
   ],
   "source": [
    "x_score = []\n",
    "final_cv_train = np.zeros(len(train_label))\n",
    "final_cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "for s in range(16):\n",
    "    cv_train = np.zeros(len(train_label))\n",
    "    cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "    # seed별\n",
    "    params['seed'] = s\n",
    "\n",
    "    # true\n",
    "    if cv_only:\n",
    "        kf = kfold.split(X, train_label)\n",
    "\n",
    "        best_trees = []\n",
    "        fold_scores = []\n",
    "\n",
    "        for i, (train_fold, validate) in enumerate(kf):\n",
    "            X_train, X_validate, label_train, label_validate = \\\n",
    "                X[train_fold, :], X[validate, :], train_label[train_fold], train_label[validate]\n",
    "            dtrain = lgbm.Dataset(X_train, label_train)\n",
    "            dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "            bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, verbose_eval=100,\n",
    "                            early_stopping_rounds=100)\n",
    "            best_trees.append(bst.best_iteration)\n",
    "            cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "            cv_train[validate] += bst.predict(X_validate)\n",
    "\n",
    "            score = Gini(label_validate, cv_train[validate])\n",
    "            print(score)\n",
    "            fold_scores.append(score)\n",
    "\n",
    "        cv_pred /= NFOLDS\n",
    "        final_cv_train += cv_train\n",
    "        final_cv_pred += cv_pred\n",
    "\n",
    "        print(\"cv score:\")\n",
    "        print(Gini(train_label, cv_train))\n",
    "        \n",
    "        # seed별 평균\n",
    "        print(\"current score:\", Gini(train_label, final_cv_train / (s + 1.)), s+1)        \n",
    "        print(fold_scores)\n",
    "        print(best_trees, np.mean(best_trees))\n",
    "\n",
    "        x_score.append(Gini(train_label, cv_train))\n",
    "\n",
    "print(x_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': test_id, 'target': final_cv_pred / 16.}).to_csv('./lgbm3_pred_avg.csv', index=False)\n",
    "pd.DataFrame({'id': train_id, 'target': final_cv_train / 16.}).to_csv('./lgbm3_pred_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 결과\n",
    "\n",
    "- private: 0.29097\n",
    "- public: 0.28525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은메달이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
